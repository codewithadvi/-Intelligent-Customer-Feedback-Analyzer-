# -*- coding: utf-8 -*-
"""Untitled115 (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11G_ybvJ9bc77RzCrE-8S5kyf4RkCwEvh
"""



!pip install datasets

!pip install bertopic --quiet
!pip install sentence-transformers --quiet

!kaggle datasets download -d kritanjalijain/amazon-reviews -f train.csv
!kaggle datasets download -d kritanjalijain/amazon-reviews -f test.csv

import zipfile

# Extract train.csv.zip
with zipfile.ZipFile("train.csv.zip", "r") as zip_ref:
    zip_ref.extractall(".")  # This will extract train.csv into the current directory

# Extract test.csv.zip
with zipfile.ZipFile("test.csv.zip", "r") as zip_ref:
    zip_ref.extractall(".")  # This will extract test.csv into the current directory

import pandas as pd

# Limit the number of rows to load (adjust nrows as needed)
nrows_train = 20000  # load only 20,000 rows from train.csv
nrows_test  = 5000   # load only 5,000 rows from test.csv

# The train CSV is expected to have three columns: (label, title, review)
# The test CSV is expected to have two columns: (label, review)
train_df = pd.read_csv("train.csv", header=None, names=['label', 'title', 'review'], nrows=nrows_train)
test_df  = pd.read_csv("test.csv", header=None, names=['label', 'review'], nrows=nrows_test)

# Save these DataFrames as normal CSV files for future use
train_df.to_csv("train_normal.csv", index=False)
test_df.to_csv("test_normal.csv", index=False)

# Quick preview of the datasets
print("Train Data Preview:")
print(train_df.head())
print("\nTest Data Preview:")
print(test_df.head())

!pip install contractions

!pip install pandarallel

import nltk
nltk.download('punkt_tab')

import pandas as pd
import re
import contractions
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns
from pandarallel import pandarallel

# Download stopwords and tokenizer
nltk.download('punkt')
nltk.download('stopwords')

# Initialize parallel processing
pandarallel.initialize(progress_bar=True)

# Load stopwords set (faster lookup)
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = text.lower()
    text = contractions.fix(text)  # Expand contractions
    text = re.sub(r"[^a-zA-Z\s]", "", text)  # Remove non-alphabetic characters
    text = " ".join(word for word in word_tokenize(text) if word not in stop_words)
    return text

# Fill missing values
train_df['review'] = train_df['review'].fillna("")
test_df['review'] = test_df['review'].fillna("")

# Apply preprocessing in parallel
train_df['text'] = train_df['review'].parallel_apply(clean_text)
test_df['text'] = test_df['review'].parallel_apply(clean_text)

# Visualization: Label distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='label', data=train_df, palette="viridis")
plt.title("Label Distribution in Training Data")
plt.xlabel("Label")
plt.ylabel("Frequency")
plt.show()

pip install emoji

from wordcloud import WordCloud
from collections import Counter
import numpy as np
import seaborn as sns
import emoji

# 1️⃣ Word Cloud for Most Common Words
def plot_wordcloud(text_series, title):
    text = " ".join(text_series)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(title, fontsize=14)
    plt.show()

print("Generating Word Clouds...")
plot_wordcloud(train_df['text'], "Most Common Words in Reviews")

# 2️⃣ Top Bigrams
from sklearn.feature_extraction.text import CountVectorizer

def plot_ngrams(text_series, n=2, top_n=10):
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english')
    ngrams = vec.fit_transform(text_series)
    ngram_counts = Counter(zip(*ngrams.nonzero()))  # Get n-gram counts

    # Get top n n-grams
    top_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]
    labels, values = zip(*[(vec.get_feature_names_out()[idx[1]], count) for idx, count in top_ngrams])

    plt.figure(figsize=(10, 5))
    sns.barplot(x=list(values), y=list(labels), palette="viridis")
    plt.xlabel("Frequency")
    plt.ylabel(f"Top {n}-grams")
    plt.title(f"Top {n}-grams in Reviews")
    plt.show()

print("Generating Bigram Visualization...")
plot_ngrams(train_df['text'], n=2)  # Plot bigrams

# 3️⃣ Review Length Distribution
train_df["word_count"] = train_df["text"].apply(lambda x: len(x.split()))
plt.figure(figsize=(8,5))
sns.histplot(train_df["word_count"], bins=30, kde=True, color="purple")
plt.title("Distribution of Review Lengths")
plt.xlabel("Number of Words in Review")
plt.ylabel("Frequency")
plt.show()

# 4️⃣ Emoji Usage Before Preprocessing
train_df["emoji_count"] = train_df["review"].apply(lambda x: sum(1 for char in x if emoji.is_emoji(char)))
plt.figure(figsize=(8,5))
sns.histplot(train_df["emoji_count"], bins=10, kde=True, color="orange")
plt.title("Distribution of Emoji Usage in Reviews")
plt.xlabel("Number of Emojis in Review")
plt.ylabel("Frequency")
plt.show()

!pip install datasets transformers evaluate --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from datasets import Dataset
import evaluate
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments

# Load and preprocess data
df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df['label'] = df['label'] - 1  # Map labels: 1->0, 2->1
df = df[['text','label']]
# (Optional) Subsample to speed up training; adjust sample size as needed
df = df.sample(n=20000, random_state=42)

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# Convert to Hugging Face Datasets and remove extraneous columns
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
cols_to_remove_train = [col for col in train_dataset.column_names if col not in ['text','label']]
cols_to_remove_val = [col for col in val_dataset.column_names if col not in ['text','label']]
train_dataset = train_dataset.remove_columns(cols_to_remove_train)
val_dataset = val_dataset.remove_columns(cols_to_remove_val)
train_dataset.set_format("torch")
val_dataset.set_format("torch")

# Tokenization: reduce max_length to 128 to speed up training
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

#Load model and set training arguments with modifications for speed
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,           # Less frequent logging to reduce overhead
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,                  # Enable mixed precision training
    report_to="none"            # Disable logging to WandB or others
)

from evaluate import load
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score, jaccard_score

accuracy_metric = load("accuracy")
f1_metric = load("f1")
precision_metric = load("precision")
recall_metric = load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    precision = precision_metric.compute(predictions=predictions, references=labels, average="weighted")
    recall = recall_metric.compute(predictions=predictions, references=labels, average="weighted")
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="weighted")

    # Compute AUC-ROC and AUC-PR (only if it's a binary classification task)
    try:
        auc_roc = roc_auc_score(labels, logits, multi_class="ovr", average="weighted")
        auc_pr = average_precision_score(labels, logits, average="weighted")
    except ValueError:
        auc_roc, auc_pr = None, None  # Avoid errors for non-binary cases

    # Compute IoU (Jaccard Score)
    iou = jaccard_score(labels, predictions, average="weighted")

    return {
        "accuracy": accuracy["accuracy"],
        "precision": precision["precision"],
        "recall": recall["recall"],
        "f1": f1["f1"],
        "auc_roc": auc_roc,
        "auc_pr": auc_pr,
        "iou": iou
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()
eval_result = trainer.evaluate()
print("Evaluation results:", eval_result)

predictions = trainer.predict(val_dataset)
preds = np.argmax(predictions.predictions, axis=-1)

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast

# Save the trained model
model.save_pretrained("saved_model")

# Save the tokenizer
tokenizer.save_pretrained("saved_model")

from joblib import dump

# Save tokenizer
tokenizer.save_pretrained("./saved_model")

# Save model
model.save_pretrained("./saved_model")

# Save using joblib (Optional)
dump(model, "distilbert_model.joblib")
dump(tokenizer, "distilbert_tokenizer.joblib")

from bertopic import BERTopic
import joblib

# Load the preprocessed text
df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')

# Train BERTopic model
topic_model = BERTopic(language="english")
topics, _ = topic_model.fit_transform(df['text'])

# Save the trained topic model
joblib.dump(topic_model, "bertopic_model.joblib")

print("BERTopic model saved successfully!")

import joblib

# Load the saved topic model
topic_model = joblib.load("bertopic_model.joblib")

# Test on a new text
new_texts = ["This laptop has amazing battery life and performance."]
new_topics, _ = topic_model.transform(new_texts)

print("Predicted topic:", new_topics)

!pip install streamlit
import streamlit as st
import joblib

# Load saved topic model
topic_model = joblib.load("bertopic_model.joblib")

st.title("Topic Modeling with BERTopic")

# User input
user_input = st.text_area("Enter a review text:")

if st.button("Predict Topic"):
    if user_input:
        topics, _ = topic_model.transform([user_input])
        st.write(f"Predicted Topic: {topics[0]}")
    else:
        st.write("Please enter some text.")

!pip install bertopic
!pip install sentence-transformers

import pandas as pd
import joblib
from bertopic import BERTopic

df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]  # We only need the cleaned text

# Initialize BERTopic model
topic_model = BERTopic(language="english")

# Fit the model to our dataset
topics, probs = topic_model.fit_transform(df['text'])

# Save the trained model for later use
joblib.dump(topic_model, "bertopic_model.joblib")
print("BERTopic model saved successfully!")

topic_model.get_topic_info().head(10)  # View the top 10 topics

topic_model.get_topic(5)  # Replace 5 with any topic ID

topic_model.get_representative_docs(5)  # Replace 5 with the topic ID

topic_model.visualize_barchart(top_n_topics=10)

predicted_topics, confidence_scores = topic_model.transform(["This is a test review"])
print("Predicted Topic:", predicted_topics, "Confidence Score:", confidence_scores)

topic_model = BERTopic(vectorizer_model=CountVectorizer(ngram_range=(1, 2)))

print(df.columns)

from bertopic import BERTopic

# Ensure 'docs' is correctly defined
docs = df['text'].tolist()

# Initialize and fit BERTopic model
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)

# Now reduce topics
topic_model.reduce_topics(docs, nr_topics=50)

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
topic_model = BERTopic(embedding_model=embedding_model)

from bertopic import BERTopic

# Ensure 'docs' is correctly defined
docs = df['text'].tolist()

# Initialize and fit BERTopic model
topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)  # FITTING THE MODEL

# Now, visualization will work
topic_model.visualize_barchart(top_n_topics=10)

if topic_model.topics_ is None:
    print("Error: Model is not fitted yet. Please fit it first using `.fit_transform()`.")
else:
    topic_model.visualize_barchart(top_n_topics=10)

topic_model.visualize_heatmap()

import pandas as pd
import numpy as np
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

# Load dataset (Ensure 'text' column exists in df)
df = pd.read_csv('/content/test_normal.csv')  # Replace with actual dataset path

# Check if 'text' column exists, if not, create it from 'review'
if 'text' not in df.columns:
    df['text'] = df['review']  # Assuming 'review' column contains the text data

# Perform Sentiment Analysis
sia = SentimentIntensityAnalyzer()
df['sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(str(x))['compound'])
df['sentiment_label'] = df['sentiment'].apply(lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))

# Create the 'date' column BEFORE separating the data
if 'date' not in df.columns:
    df['date'] = pd.date_range(start="2023-01-01", periods=len(df), freq="D")

# Reset index to ensure it starts from 0 and is consecutive
df = df.reset_index(drop=True)  # Resetting the index

# Separate data based on sentiment
df_positive = df[df['sentiment_label'] == 'positive']
df_negative = df[df['sentiment_label'] == 'negative']
df_neutral = df[df['sentiment_label'] == 'neutral']

# Topic Modeling for each sentiment category
def perform_topic_modeling(texts, num_topics=50):
    if len(texts) == 0:
        return None, None  # Skip empty datasets
    topic_model = BERTopic()
    topics, probs = topic_model.fit_transform(texts)
    return topic_model, topics

# Apply topic modeling separately
texts_positive = df_positive['text'].dropna().tolist()
texts_negative = df_negative['text'].dropna().tolist()
texts_neutral = df_neutral['text'].dropna().tolist()

model_positive, topics_positive = perform_topic_modeling(texts_positive)
model_negative, topics_negative = perform_topic_modeling(texts_negative)
model_neutral, topics_neutral = perform_topic_modeling(texts_neutral)

# Reduce topics if necessary
if model_positive:
    model_positive.reduce_topics(texts_positive, nr_topics=20)
if model_negative:
    model_negative.reduce_topics(texts_negative, nr_topics=20)
if model_neutral:
    model_neutral.reduce_topics(texts_neutral, nr_topics=20)

# Visualizations
if model_positive:
    model_positive.visualize_barchart(top_n_topics=10)
if model_negative:
    model_negative.visualize_barchart(top_n_topics=10)
if model_neutral:
    model_neutral.visualize_barchart(top_n_topics=10)

# Topics Over Time
# Resetting the index of the filtered dataframes is crucial
df_positive = df_positive.reset_index(drop=True)
df_negative = df_negative.reset_index(drop=True)
df_neutral = df_neutral.reset_index(drop=True)

if model_positive:
    topics_over_time_pos = model_positive.topics_over_time(texts_positive, df_positive['date'])
    model_positive.visualize_topics_over_time(topics_over_time_pos)
if model_negative:
    topics_over_time_neg = model_negative.topics_over_time(texts_negative, df_negative['date'])
    model_negative.visualize_topics_over_time(topics_over_time_neg)
if model_neutral:
    topics_over_time_neu = model_neutral.topics_over_time(texts_neutral, df_neutral['date'])
    model_neutral.visualize_topics_over_time(topics_over_time_neu)

# -------------------------#
# INSTALL REQUIRED LIBRARIES (if not already installed)
# -------------------------#
!pip install bertopic sentence-transformers nltk pandarallel joblib

# -------------------------#
# IMPORT LIBRARIES
# -------------------------#
import pandas as pd
import joblib
from bertopic import BERTopic
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from pandarallel import pandarallel

# -------------------------#
# DOWNLOAD NLTK RESOURCES
# -------------------------#
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Initialize pandarallel for faster processing (optional)
pandarallel.initialize(progress_bar=True)

# -------------------------#
# LOAD AND PREPROCESS DATASET
# -------------------------#
# Load the dataset (assuming 'train_normal.csv' exists)
df = pd.read_csv("train_normal.csv")

# Create a 'text' column by merging title and review
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')

# -------------------------#
# SENTIMENT ANALYSIS
# -------------------------#
# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Compute the compound sentiment score for each review
df['sentiment_score'] = df['text'].apply(lambda x: sia.polarity_scores(x)['compound'])

# Label the reviews: positive if score > 0.05, negative if score < -0.05, else neutral
df['sentiment_label'] = df['sentiment_score'].apply(
    lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral')
)

# Optional: View sentiment distribution
print("Sentiment Distribution:")
print(df['sentiment_label'].value_counts())

# -------------------------#
# SPLIT DATA BY SENTIMENT
# -------------------------#
# For separate topic models, filter positive and negative reviews
df_positive = df[df['sentiment_label'] == 'positive']
df_negative = df[df['sentiment_label'] == 'negative']

# -------------------------#
# TRAIN BERTopic MODELS ON EACH SUBSET
# -------------------------#
# Train BERTopic on positive reviews
positive_topic_model = BERTopic(language="english")
topics_pos, probs_pos = positive_topic_model.fit_transform(df_positive['text'].tolist())

# Train BERTopic on negative reviews
negative_topic_model = BERTopic(language="english")
topics_neg, probs_neg = negative_topic_model.fit_transform(df_negative['text'].tolist())

# Optional: Print top topics for each model
print("Positive Topic Model - Top 10 Topics:")
print(positive_topic_model.get_topic_info().head(10))
print("Negative Topic Model - Top 10 Topics:")
print(negative_topic_model.get_topic_info().head(10))

# -------------------------#
# SAVE THE MODELS USING JOBLIB
# -------------------------#
joblib.dump(positive_topic_model, "bertopic_positive.joblib")
joblib.dump(negative_topic_model, "bertopic_negative.joblib")

print("BERTopic models for positive and negative sentiments saved successfully!")

print(df.head())  # Check column names and structure
print(df.columns)  # See exact column names
print(df.index)  # Check the index type

!pip install bertopic sentence-transformers transformers

import pandas as pd
import joblib
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load dataset
df = pd.read_csv("train_normal.csv")

# Ensure text is properly processed
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]  # Keeping only the required column

# Sentiment Analysis Model
sentiment_model = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

# Apply sentiment analysis
df['sentiment'] = df['text'].apply(lambda x: sentiment_model(x)[0]['label'].lower())

# Display sentiment distribution
print(df['sentiment'].value_counts())

# Topic Modeling with BERTopic
embedding_model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
topic_model = BERTopic(embedding_model=embedding_model)

# Perform topic modeling separately for Positive & Negative reviews
df_pos = df[df['sentiment'] == 'positive']
df_neg = df[df['sentiment'] == 'negative']

docs_pos = df_pos['text'].tolist()
docs_neg = df_neg['text'].tolist()

# Fit topic models separately
topics_pos, _ = topic_model.fit_transform(docs_pos)
topics_neg, _ = topic_model.fit_transform(docs_neg)

# Save the trained models
joblib.dump(topic_model, "bertopic_model.joblib")

# Generate visualizations
topic_model.visualize_barchart(top_n_topics=10)

# Word Cloud for dominant topics in Positive & Negative Sentiments
def plot_wordcloud(texts, title):
    text = " ".join(texts)
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(title)
    plt.show()

plot_wordcloud(docs_pos, "Positive Sentiment Topics")
plot_wordcloud(docs_neg, "Negative Sentiment Topics")

# Predict new review sentiment + topic
new_review = ["This movie had an amazing story and breathtaking visuals!"]
sentiment = sentiment_model(new_review)[0]['label'].lower()
predicted_topic, confidence_score = topic_model.transform(new_review)

print(f"Review Sentiment: {sentiment}")
print(f"Predicted Topic: {predicted_topic}, Confidence: {confidence_score}")

import joblib
joblib.dump(topic_model, "bertopic_model.joblib")

topic_model = joblib.load("bertopic_model.joblib")

import pandas as pd
import joblib
from bertopic import BERTopic
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load pre-trained BERTopic model
topic_model = joblib.load("bertopic_model.joblib")

# Load dataset
df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]  # Only keep the cleaned text

# Extract topics for all reviews
topics, probs = topic_model.transform(df['text'])

# Convert topics to DataFrame
df['topic'] = topics
df['topic_probability'] = probs

"""Recommendation System"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Load pre-trained sentence transformer model for better similarity matching
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate topic embeddings (assuming topic_model is already trained)
topic_embeddings = topic_model.topic_embeddings_

# Compute cosine similarity between topic vectors
topic_similarities = cosine_similarity(topic_embeddings)

# Precompute embeddings for all reviews in the dataset
df['embedding'] = df['text'].apply(lambda x: model.encode(x))

# Function to recommend similar reviews
def recommend_similar_reviews(review_text, top_n=5, topic_threshold=0.5):
    # Get the most relevant topic for the input review
    input_topic_dist, _ = topic_model.transform([review_text])
    input_topic = np.argmax(input_topic_dist)  # Most probable topic

    # Get similarity scores for all topics
    topic_scores = topic_similarities[input_topic]

    # Filter similar topics above the threshold
    similar_topics = [t for t in np.argsort(topic_scores)[::-1] if topic_scores[t] > topic_threshold]

    # Find reviews from relevant topics
    candidate_reviews = df[df['topic'].isin(similar_topics)]

    # Compute similarity based on sentence embeddings
    input_embedding = model.encode(review_text)
    candidate_reviews['similarity'] = candidate_reviews['embedding'].apply(lambda x: cosine_similarity([input_embedding], [x])[0][0])

    # Return top N most similar reviews
    return candidate_reviews[['text', 'topic', 'similarity']].sort_values(by='similarity', ascending=False).head(top_n)

# Example usage
test_review = "This laptop has amazing battery life and performance."
recommended_reviews = recommend_similar_reviews(test_review)
print(recommended_reviews)

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import joblib
from bertopic import BERTopic

# -------------------------#
# 1. Load your trained BERTopic model and dataset
# -------------------------#
topic_model = joblib.load("bertopic_model.joblib")  # Ensure this file exists

df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]

# -------------------------#
# 2. Ensure each review has an assigned topic
# -------------------------#
if 'topic' not in df.columns:
    topics, _ = topic_model.transform(df['text'])
    df['topic'] = topics

# -------------------------#
# 3. Precompute document embeddings using SentenceTransformer
# -------------------------#
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
if 'embedding' not in df.columns:
    df['embedding'] = df['text'].apply(lambda x: embedding_model.encode(x, convert_to_numpy=True))

# -------------------------#
# 4. Compute cosine similarity between topic embeddings (for topic filtering)
# -------------------------#
topic_embeddings = topic_model.topic_embeddings_
if topic_embeddings is None or topic_embeddings.size == 0:
    topic_similarities = None
else:
    topic_similarities = cosine_similarity(topic_embeddings)

# -------------------------#
# 5. Define the enhanced recommendation function with debug prints and fallback
# -------------------------#
def refined_recommend_similar_reviews_with_keywords(review_text, top_n=5, topic_threshold=0.4,
                                                      min_similarity=0.4, alpha=0.7, top_keyword_count=10):
    """
    Recommend similar reviews using a three-pronged approach:
      1. Topic filtering: Candidate reviews must belong to topics similar (by cosine similarity of topic embeddings)
         to the input review's topic.
      2. Document-level similarity: Compute cosine similarity between the input review and candidate reviews.
      3. Keyword overlap: Fraction of top keywords (from the input topic) present in the candidate review.

    A combined score (weighted sum) is used for ranking.

    Parameters:
      review_text (str): Input review.
      top_n (int): Number of recommendations.
      topic_threshold (float): Minimum cosine similarity between topics for filtering.
      min_similarity (float): Minimum cosine similarity between document embeddings.
      alpha (float): Weight for cosine similarity (1 - alpha for keyword overlap).
      top_keyword_count (int): Number of top keywords to extract from the input topic.

    Returns:
      DataFrame with recommended reviews.
    """
    # 1. Get the assigned topic for the input review.
    input_topics, _ = topic_model.transform([review_text])
    input_topic = input_topics[0]
    if isinstance(input_topic, list):
        input_topic = input_topic[0]
    print("Input review assigned topic:", input_topic)

    # 2. Extract top keywords for the input topic.
    topic_keywords = topic_model.get_topic(input_topic)
    top_keywords = [word for word, weight in topic_keywords[:top_keyword_count]] if topic_keywords else []
    print("Top keywords for input topic:", top_keywords)

    # 3. Determine similar topics using topic-level cosine similarity.
    if topic_similarities is not None and topic_similarities.shape[0] > input_topic:
        topic_scores = topic_similarities[input_topic]
        similar_topics = [t for t in np.argsort(topic_scores)[::-1] if topic_scores[t] >= topic_threshold]
    else:
        similar_topics = [input_topic]
    print("Similar topics (threshold {}):".format(topic_threshold), similar_topics)

    # 4. Filter candidate reviews by similar topics.
    candidate_reviews = df[df['topic'].isin(similar_topics)].copy()
    print("Candidate reviews after topic filtering:", len(candidate_reviews))

    # 5. Compute embedding for the input review.
    input_embedding = embedding_model.encode(review_text, convert_to_numpy=True)

    # 6. Compute cosine similarity for each candidate review.
    candidate_reviews['cosine_similarity'] = candidate_reviews['embedding'].apply(
        lambda x: cosine_similarity([input_embedding], [x])[0][0]
    )

    # 7. Compute keyword overlap score for each candidate review.
    def keyword_overlap(text):
        tokens = set(text.split())
        if top_keywords:
            return len(tokens.intersection(set(top_keywords))) / len(top_keywords)
        return 0.0
    candidate_reviews['keyword_overlap'] = candidate_reviews['text'].apply(keyword_overlap)

    # 8. Compute a combined score.
    candidate_reviews['combined_score'] = alpha * candidate_reviews['cosine_similarity'] + (1 - alpha) * candidate_reviews['keyword_overlap']

    # 9. Filter candidates by minimum cosine similarity.
    filtered_candidates = candidate_reviews[candidate_reviews['cosine_similarity'] >= min_similarity]
    print("Candidates after similarity filtering:", len(filtered_candidates))

    # Fallback: If no candidate remains, use all reviews sorted by cosine similarity.
    if filtered_candidates.empty:
        print("No candidate reviews met the minimum similarity threshold. Falling back to all reviews.")
        df['cosine_similarity'] = df['embedding'].apply(lambda x: cosine_similarity([input_embedding], [x])[0][0])
        filtered_candidates = df[df['cosine_similarity'] >= min_similarity].copy()
        filtered_candidates['combined_score'] = filtered_candidates['cosine_similarity']  # fallback only cosine similarity

    # 10. Return top_n results sorted by combined score.
    return filtered_candidates[['text', 'topic', 'cosine_similarity', 'keyword_overlap', 'combined_score']].sort_values(
        by='combined_score', ascending=False).head(top_n)

# -------------------------#
# 6. Example Usage
# -------------------------#
test_review = "This laptop has amazing battery life and performance."
recommended_reviews = refined_recommend_similar_reviews_with_keywords(
    test_review, top_n=5, topic_threshold=0.4, min_similarity=0.4, alpha=0.7, top_keyword_count=10
)
print("Enhanced Recommended Reviews:")
print(recommended_reviews)

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import joblib
from bertopic import BERTopic
import matplotlib.pyplot as plt

# -------------------------#
# 1. Load your trained BERTopic model and dataset
# -------------------------#
topic_model = joblib.load("bertopic_model.joblib")  # Ensure this file exists

df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]

# -------------------------#
# 2. Ensure each review has an assigned topic
# -------------------------#
if 'topic' not in df.columns:
    topics, _ = topic_model.transform(df['text'])
    df['topic'] = topics

# -------------------------#
# 3. Precompute document embeddings using SentenceTransformer
# -------------------------#
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
if 'embedding' not in df.columns:
    df['embedding'] = df['text'].apply(lambda x: embedding_model.encode(x, convert_to_numpy=True))

# -------------------------#
# 4. Compute cosine similarity between topic embeddings (for topic filtering)
# -------------------------#
topic_embeddings = topic_model.topic_embeddings_
if topic_embeddings is None or topic_embeddings.size == 0:
    topic_similarities = None
else:
    topic_similarities = cosine_similarity(topic_embeddings)

# -------------------------#
# 5. Define a function to generate a recommendation message based on top keywords
# -------------------------#
def generate_recommendation_message(top_keywords):
    """
    Given a list of top keywords from the input topic, generate a recommendation statement.
    Expand this mapping as needed.
    """
    if "delivery" in top_keywords and "slow" in top_keywords:
        return "Optimize your delivery logistics."
    elif "price" in top_keywords:
        return "Review your pricing strategy."
    elif "quality" in top_keywords:
        return "Focus on improving product quality."
    elif "customer" in top_keywords:
        return "Improve customer service and support."
    else:
        return "No specific recommendation found."

# -------------------------#
# 6. Define the enhanced recommendation function with keyword overlap and recommendation message
# -------------------------#
def refined_recommend_similar_reviews_with_keywords(review_text, top_n=5, topic_threshold=0.4,
                                                      min_similarity=0.4, alpha=0.7, top_keyword_count=10):
    """
    Recommend similar reviews using a three-pronged approach:
      1. Topic filtering: Candidate reviews must belong to topics similar (by cosine similarity of topic embeddings)
         to the input review's topic.
      2. Document-level similarity: Compute cosine similarity between the input review and candidate reviews.
      3. Keyword overlap: Fraction of top keywords (from the input topic) present in the candidate review.

    A combined score (weighted sum) is used for ranking, and a recommendation statement is generated
    based on the top keywords.

    Parameters:
      review_text (str): Input review.
      top_n (int): Number of recommendations.
      topic_threshold (float): Minimum cosine similarity between topics for filtering.
      min_similarity (float): Minimum cosine similarity between document embeddings.
      alpha (float): Weight for cosine similarity (1 - alpha for keyword overlap).
      top_keyword_count (int): Number of top keywords to extract from the input topic.

    Returns:
      A tuple of (DataFrame with recommended reviews, recommendation message).
    """
    # 1. Get the assigned topic for the input review.
    input_topics, _ = topic_model.transform([review_text])
    input_topic = input_topics[0]
    if isinstance(input_topic, list):
        input_topic = input_topic[0]
    print("Input review assigned topic:", input_topic)

    # 2. Extract top keywords for the input topic.
    topic_keywords = topic_model.get_topic(input_topic)
    top_keywords = [word for word, weight in topic_keywords[:top_keyword_count]] if topic_keywords else []
    print("Top keywords for input topic:", top_keywords)

    # Generate a recommendation message based on the top keywords.
    recommendation_message = generate_recommendation_message(top_keywords)
    print("Recommendation Message:", recommendation_message)

    # 3. Determine similar topics using topic-level cosine similarity.
    if topic_similarities is not None and topic_similarities.shape[0] > input_topic:
        topic_scores = topic_similarities[input_topic]
        similar_topics = [t for t in np.argsort(topic_scores)[::-1] if topic_scores[t] >= topic_threshold]
    else:
        similar_topics = [input_topic]
    print("Similar topics (threshold {}):".format(topic_threshold), similar_topics)

    # 4. Filter candidate reviews by similar topics.
    candidate_reviews = df[df['topic'].isin(similar_topics)].copy()
    print("Candidate reviews after topic filtering:", len(candidate_reviews))

    # 5. Compute the embedding for the input review.
    input_embedding = embedding_model.encode(review_text, convert_to_numpy=True)

    # 6. Compute cosine similarity for each candidate review.
    candidate_reviews['cosine_similarity'] = candidate_reviews['embedding'].apply(
        lambda x: cosine_similarity([input_embedding], [x])[0][0]
    )

    # 7. Compute keyword overlap score for each candidate review.
    def keyword_overlap(text):
        tokens = set(text.split())
        if top_keywords:
            return len(tokens.intersection(set(top_keywords))) / len(top_keywords)
        return 0.0
    candidate_reviews['keyword_overlap'] = candidate_reviews['text'].apply(keyword_overlap)

    # 8. Compute a combined score.
    candidate_reviews['combined_score'] = alpha * candidate_reviews['cosine_similarity'] + (1 - alpha) * candidate_reviews['keyword_overlap']

    # 9. Filter candidates by minimum cosine similarity.
    filtered_candidates = candidate_reviews[candidate_reviews['cosine_similarity'] >= min_similarity]
    print("Candidates after similarity filtering:", len(filtered_candidates))

    # Fallback: If no candidate remains, use all reviews sorted by cosine similarity.
    if filtered_candidates.empty:
        print("No candidate reviews met the minimum similarity threshold. Falling back to all reviews.")
        candidate_reviews['cosine_similarity'] = candidate_reviews['embedding'].apply(
            lambda x: cosine_similarity([input_embedding], [x])[0][0]
        )
        filtered_candidates = candidate_reviews.copy()
        filtered_candidates['combined_score'] = filtered_candidates['cosine_similarity']  # fallback only cosine similarity

    # 10. Return top_n results sorted by combined score and the recommendation message.
    return filtered_candidates[['text', 'topic', 'cosine_similarity', 'keyword_overlap', 'combined_score']].sort_values(
        by='combined_score', ascending=False).head(top_n), recommendation_message

# -------------------------#
# 7. Example Usage and Visualizations
# -------------------------#
test_review = "This laptop has amazing battery life and performance."
recommended_reviews, rec_message = refined_recommend_similar_reviews_with_keywords(
    test_review, top_n=5, topic_threshold=0.4, min_similarity=0.4, alpha=0.7, top_keyword_count=10
)
print("Enhanced Recommended Reviews:")
print(recommended_reviews)
print("Generated Recommendation Statement:")
print(rec_message)

# Visualization 1: Horizontal Bar Chart of Combined Scores
if not recommended_reviews.empty:
    plt.figure(figsize=(10, 6))
    plt.barh(recommended_reviews['text'], recommended_reviews['combined_score'], color='skyblue')
    plt.xlabel('Combined Score')
    plt.title('Recommended Reviews Combined Score')
    plt.gca().invert_yaxis()
    plt.show()
else:
    print("No recommendations to visualize in bar chart.")

# Visualization 2: Scatter Plot (Cosine Similarity vs. Keyword Overlap)
if not recommended_reviews.empty:
    plt.figure(figsize=(8,6))
    plt.scatter(recommended_reviews['cosine_similarity'], recommended_reviews['keyword_overlap'],
                c=recommended_reviews['combined_score'], cmap='viridis', s=100)
    plt.colorbar(label='Combined Score')
    plt.xlabel('Cosine Similarity')
    plt.ylabel('Keyword Overlap')
    plt.title('Cosine Similarity vs. Keyword Overlap for Recommended Reviews')
    plt.show()
else:
    print("No recommendations to visualize in scatter plot.")

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import joblib
from bertopic import BERTopic
import nltk
from nltk.corpus import stopwords

# Ensure stopwords are downloaded
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# -------------------------#
# 1. Load trained BERTopic model and dataset
# -------------------------#
topic_model = joblib.load("bertopic_model.joblib")  # Ensure this file exists

df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]

# -------------------------#
# 2. Ensure each review has an assigned topic
# -------------------------#
if 'topic' not in df.columns:
    topics, _ = topic_model.transform(df['text'])
    df['topic'] = topics

# -------------------------#
# 3. Precompute document embeddings using SentenceTransformer
# -------------------------#
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
if 'embedding' not in df.columns:
    df['embedding'] = df['text'].apply(lambda x: embedding_model.encode(x, convert_to_numpy=True))

# -------------------------#
# 4. Compute cosine similarity between topic embeddings
# -------------------------#
topic_embeddings = topic_model.topic_embeddings_
if topic_embeddings is None or topic_embeddings.size == 0:
    topic_similarities = None
else:
    topic_similarities = cosine_similarity(topic_embeddings)

# -------------------------#
# 5. Define Recommendation Message Generator
# -------------------------#
def generate_recommendation_message(top_keywords):
    """
    Generates a recommendation based on the most frequent meaningful keywords.
    """
    if not top_keywords:
        return "No specific recommendation found."

    # Example rule-based recommendations based on keywords
    keyword_map = {
        "delivery": "Optimize your delivery logistics.",
        "slow": "Improve processing speed and efficiency.",
        "price": "Consider offering discounts or promotions.",
        "battery": "Enhance battery performance for better user experience.",
        "service": "Focus on improving customer support and response time."
    }

    # Find the best matching recommendation
    for keyword in top_keywords:
        if keyword in keyword_map:
            return keyword_map[keyword]

    return "No specific recommendation found."

# -------------------------#
# 6. Define Enhanced Recommendation Function
# -------------------------#
def refined_recommend_similar_reviews_with_keywords(review_text, top_n=5, topic_threshold=0.4,
                                                      min_similarity=0.4, alpha=0.7, top_keyword_count=10):
    """
    Recommends similar reviews based on:
      1. Topic filtering (similar topics to input review).
      2. Document-level similarity (cosine similarity).
      3. Keyword overlap.
    """

    # 1. Get the assigned topic for the input review.
    input_topics, _ = topic_model.transform([review_text])
    input_topic = input_topics[0]
    if isinstance(input_topic, list):
        input_topic = input_topic[0]
    print("Input review assigned topic:", input_topic)

    # 2. Handle outliers (-1 topic)
    if input_topic == -1:
        if topic_similarities is not None:
            similar_topics = np.argsort(topic_similarities.mean(axis=0))[-5:]  # Pick top 5 closest topics
            input_topic = similar_topics[0]  # Choose the most similar topic
            print("Input topic was an outlier. Using closest topic:", input_topic)
        else:
            return "This review is an outlier. No similar topics found."

    # 3. Extract top meaningful keywords for the input topic.
    topic_keywords = topic_model.get_topic(input_topic)
    top_keywords = [word for word, weight in topic_keywords[:top_keyword_count] if word.lower() not in stop_words] if topic_keywords else []
    print("Top keywords for input topic:", top_keywords)

    # 4. Generate recommendation message
    recommendation_message = generate_recommendation_message(top_keywords)
    print("Recommendation Message:", recommendation_message)

    # 5. Determine similar topics using topic-level cosine similarity.
    if topic_similarities is not None and topic_similarities.shape[0] > input_topic:
        topic_scores = topic_similarities[input_topic]
        similar_topics = [t for t in np.argsort(topic_scores)[::-1] if topic_scores[t] >= topic_threshold]
    else:
        similar_topics = [input_topic]
    print("Similar topics (threshold {}):".format(topic_threshold), similar_topics)

    # 6. Filter candidate reviews by similar topics.
    candidate_reviews = df[df['topic'].isin(similar_topics)].copy()
    print("Candidate reviews after topic filtering:", len(candidate_reviews))

    # 7. Compute embedding for the input review.
    input_embedding = embedding_model.encode(review_text, convert_to_numpy=True)

    # 8. Compute cosine similarity for each candidate review.
    candidate_reviews['cosine_similarity'] = candidate_reviews['embedding'].apply(
        lambda x: cosine_similarity([input_embedding], [x])[0][0]
    )

    # 9. Compute keyword overlap score for each candidate review.
    def keyword_overlap(text):
        tokens = set(text.split())
        if top_keywords:
            return len(tokens.intersection(set(top_keywords))) / len(top_keywords)
        return 0.0
    candidate_reviews['keyword_overlap'] = candidate_reviews['text'].apply(keyword_overlap)

    # 10. Compute a combined score.
    candidate_reviews['combined_score'] = alpha * candidate_reviews['cosine_similarity'] + (1 - alpha) * candidate_reviews['keyword_overlap']

    # 11. Filter candidates by minimum cosine similarity.
    filtered_candidates = candidate_reviews[candidate_reviews['cosine_similarity'] >= min_similarity]
    print("Candidates after similarity filtering:", len(filtered_candidates))

    # Fallback: If no candidate remains, use all reviews sorted by cosine similarity.
    if filtered_candidates.empty:
        print("No candidate reviews met the minimum similarity threshold. Falling back to all reviews.")
        df['cosine_similarity'] = df['embedding'].apply(lambda x: cosine_similarity([input_embedding], [x])[0][0])
        filtered_candidates = df[df['cosine_similarity'] >= min_similarity].copy()
        filtered_candidates['combined_score'] = filtered_candidates['cosine_similarity']  # fallback only cosine similarity

    # 12. Return top_n results sorted by combined score.
    final_results = filtered_candidates[['text', 'topic', 'cosine_similarity', 'keyword_overlap', 'combined_score']].sort_values(
        by='combined_score', ascending=False).head(top_n)

    print("\nEnhanced Recommended Reviews:")
    print(final_results)

    return {
        "recommendations": final_results,
        "recommendation_message": recommendation_message
    }

# -------------------------#
# 7. Example Usage
# -------------------------#
test_review = "The delivery of the product was slow and the customer service was unresponsive."
result = refined_recommend_similar_reviews_with_keywords(
    test_review, top_n=5, topic_threshold=0.4, min_similarity=0.4, alpha=0.7, top_keyword_count=10
)

print("\nGenerated Recommendation Statement:")
print(result["recommendation_message"])

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import joblib
from bertopic import BERTopic
import nltk
from nltk.corpus import stopwords
from collections import Counter

# Ensure stopwords are downloaded
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# -------------------------#
# 1. Load or Train BERTopic Model
# -------------------------#
try:
    topic_model = joblib.load("bertopic_model.joblib")
    print("✅ Loaded existing BERTopic model.")
except FileNotFoundError:
    print("⚠️ BERTopic model not found. Training a new one...")
    df = pd.read_csv("train_normal.csv")
    df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
    topic_model = BERTopic(verbose=True)
    topics, _ = topic_model.fit_transform(df['text'])
    df['topic'] = topics
    joblib.dump(topic_model, "bertopic_model.joblib")
    print("✅ BERTopic model trained and saved.")

# -------------------------#
# 2. Load Dataset and Compute Embeddings
# -------------------------#
df = pd.read_csv("train_normal.csv")
df['text'] = df['title'].fillna('') + " " + df['review'].fillna('')
df = df[['text']]

if 'topic' not in df.columns:
    topics, _ = topic_model.transform(df['text'])
    df['topic'] = topics

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

if 'embedding' not in df.columns:
    df['embedding'] = df['text'].apply(lambda x: embedding_model.encode(x, convert_to_numpy=True))

# Compute topic similarities
topic_embeddings = topic_model.topic_embeddings_
topic_similarities = cosine_similarity(topic_embeddings) if topic_embeddings is not None else None

# -------------------------#
# 3. Define Recommendation Message Generator
# -------------------------#
def generate_recommendation_message(top_keywords):
    keyword_map = {
        "delivery": "Optimize your delivery logistics.",
        "slow": "Improve processing speed and efficiency.",
        "price": "Consider offering discounts or promotions.",
        "battery": "Enhance battery performance for better user experience.",
        "service": "Focus on improving customer support and response time.",
        "negative": "Ensure positive product experience and address negative feedback.",
        "harmful": "Improve product safety and compliance with health standards.",
        "defective": "Enhance product quality control to reduce defects.",
        "support": "Enhance customer support responsiveness.",
    }
    for keyword in top_keywords:
        if keyword in keyword_map:
            return keyword_map[keyword]
    return "Improve customer satisfaction by addressing common concerns."

# -------------------------#
# 4. Extract Top Keywords
# -------------------------#
def extract_top_keywords(topic_keywords, num_keywords=10):
    return [word for word, weight in topic_keywords[:num_keywords] if word.lower() not in stop_words]

# -------------------------#
# 5. Recommendation Function
# -------------------------#
def refined_recommend_similar_reviews_with_keywords(review_text, top_n=5, topic_threshold=0.4,
                                                      min_similarity=0.4, alpha=0.7, top_keyword_count=10):
    input_topics, _ = topic_model.transform([review_text])
    input_topic = input_topics[0]

    if isinstance(input_topic, list):
        input_topic = input_topic[0]

    print("Input review assigned topic:", input_topic)

    if input_topic == -1:
        if topic_similarities is not None:
            similar_topics = np.argsort(topic_similarities.mean(axis=0))[-5:]
            input_topic = similar_topics[0]
            print("Using closest topic:", input_topic)
        else:
            return "This review is an outlier. No similar topics found."

    topic_keywords = topic_model.get_topic(input_topic)
    top_keywords = extract_top_keywords(topic_keywords, top_keyword_count)
    print("Top keywords:", top_keywords)

    recommendation_message = generate_recommendation_message(top_keywords)
    print("Recommendation Message:", recommendation_message)

    if topic_similarities is not None and topic_similarities.shape[0] > input_topic:
        topic_scores = topic_similarities[input_topic]
        similar_topics = [t for t in np.argsort(topic_scores)[::-1] if topic_scores[t] >= topic_threshold]
    else:
        similar_topics = [input_topic]

    print("Similar topics:", similar_topics)

    candidate_reviews = df[df['topic'].isin(similar_topics)].copy()
    print("Candidate reviews after topic filtering:", len(candidate_reviews))

    input_embedding = embedding_model.encode(review_text, convert_to_numpy=True)

    candidate_reviews['cosine_similarity'] = candidate_reviews['embedding'].apply(
        lambda x: cosine_similarity([input_embedding], [x])[0][0]
    )

    def keyword_overlap(text):
        tokens = set(text.split())
        return len(tokens.intersection(set(top_keywords))) / len(top_keywords) if top_keywords else 0.0

    candidate_reviews['keyword_overlap'] = candidate_reviews['text'].apply(keyword_overlap)

    candidate_reviews['combined_score'] = alpha * candidate_reviews['cosine_similarity'] + (1 - alpha) * candidate_reviews['keyword_overlap']

    filtered_candidates = candidate_reviews[candidate_reviews['cosine_similarity'] >= min_similarity]
    print("Candidates after similarity filtering:", len(filtered_candidates))

    final_results = filtered_candidates[['text', 'topic', 'cosine_similarity', 'keyword_overlap', 'combined_score']].sort_values(
        by='combined_score', ascending=False).head(top_n)

    print("\nEnhanced Recommended Reviews:")
    print(final_results)

    return {
        "recommendations": final_results,
        "recommendation_message": recommendation_message
    }

# -------------------------#
# 6. Save Complete Model as Joblib
# -------------------------#
model_data = {
    "topic_model": topic_model,
    "embedding_model": embedding_model,
    "topic_similarities": topic_similarities,
    "data": df,
    "recommend_function": refined_recommend_similar_reviews_with_keywords
}

joblib.dump(model_data, "recommendation_model.joblib")
print("✅ Recommendation Model Saved as 'recommendation_model.joblib'")

# -------------------------#
# 7. Load & Test Model
# -------------------------#
model_data = joblib.load("recommendation_model.joblib")
recommend_function = model_data["recommend_function"]

test_review = "The product got delayed and the product quality was poor."
result = recommend_function(test_review)

print("\nGenerated Recommendation Statement:")
print(result["recommendation_message"])
